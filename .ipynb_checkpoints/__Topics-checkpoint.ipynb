{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./topic_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Introduction to deep learning',\n",
       "       'Course overview part 1: Motivation and topics',\n",
       "       'Course overview part 2: organization (Optional)',\n",
       "       'What is machine learning?',\n",
       "       'Broad Categories of ML Part 1: Supervised, unsupervised, reinforcement learning',\n",
       "       'Broad Categories of ML Part 2: Special cases of supervised learning',\n",
       "       'The supervised learning workflow',\n",
       "       'Necessary machine learning notation and jargon',\n",
       "       'About the practical aspects and tools used in this course',\n",
       "       'A brief history of deep learning ', 'Artificial neurons',\n",
       "       'Multilayer networks', 'The origins of deep learning',\n",
       "       'The deep learning hardware and software landscape',\n",
       "       'Current trends in deep learning', 'Perceptron lecture overview',\n",
       "       'About brains and neurons', 'The perceptron learning rule',\n",
       "       'Vectorization in python',\n",
       "       'Perceptron in python using NumPy and PyTorch',\n",
       "       'The geometric intuition behind the perceptron',\n",
       "       'Linear algebra for deep learning', 'Tensors in deep learning',\n",
       "       'Tensors in PyTorch', 'Vectors, matrices, and broadcasting',\n",
       "       'Notational conventions for neural networks',\n",
       "       'A fully connected (Linear) layer in PyTorch',\n",
       "       'Gradient descent overview ', 'Online, batch, and minibatch mode',\n",
       "       'Relation between perceptron and linear regression',\n",
       "       'An iterative training algorithm for linear regression',\n",
       "       '(Optional) Calculus refresher I: Derivatives',\n",
       "       '(Optional) Calculus refresher II: Gradients',\n",
       "       'Understanding gradient descent',\n",
       "       'Training an adaptive linear neuron (Adaline)', 'Adaline in code',\n",
       "       'Automatic differentiation in PyTorch ',\n",
       "       'Learning more about PyTorch',\n",
       "       'Understanding automatic differentiation via computation graphs',\n",
       "       'Automatic differentiation in PyTorch ',\n",
       "       'Training ADALINE with PyTorch',\n",
       "       'A closer look at the PyTorch API', 'GPU resources & Google Colab',\n",
       "       'Logistic regression ',\n",
       "       'Logistic regression as a single layer neural network',\n",
       "       'Logistic regression loss function',\n",
       "       'Logistic regression loss derivative and training',\n",
       "       'Logits and cross entropy', 'Logistic regression in PyTorch',\n",
       "       'Multinomial logistic regression and Softmax regression',\n",
       "       'OneHot encoding and multi-category cross Entropy',\n",
       "       'Softmax regression derivatives for gradient descent',\n",
       "       'Softmax regression code ', 'Multilayer perceptrons',\n",
       "       'Multilayer perceptron architecture',\n",
       "       'Nonlinear activation functions', 'Multilayer Perceptron code ',\n",
       "       'Overfitting and underfitting',\n",
       "       'Cats and dogs with custom data loaders',\n",
       "       'Custom dataLoaders in PyTorch ',\n",
       "       'Regularization methods for neural networks',\n",
       "       'Techniques for reducing overfitting',\n",
       "       'Data augmentation in PyTorch', 'Early stopping',\n",
       "       'L2 regularization for neural nets',\n",
       "       'The main concept behind dropout',\n",
       "       'Dropout co-adaptation interpretation',\n",
       "       '(Optional) Dropout ensemble interpretation', 'Dropout in PyTorch',\n",
       "       'Input mormalization and weight initialization',\n",
       "       'Input normalization', 'How BatchNorm works',\n",
       "       'BatchNorm in PyTorch', 'Why BatchNorm Works',\n",
       "       'Weight initialization',\n",
       "       'Xavier Glorot and Kaiming He initialization',\n",
       "       'Weight initialization in PyTorch ',\n",
       "       'Improving gradient descent based optimization',\n",
       "       'Learning rate decay in a nutshell',\n",
       "       'Learning Rate Schedulers in PyTorch', 'SGD with Momentum',\n",
       "       'Adam: Combining Adaptive Learning Rates and Momentum',\n",
       "       'Choosing Different Optimizers in PyTorch',\n",
       "       'Additional Topics and Research on Optimization Algorithms',\n",
       "       'Introduction to Convolutional Networks ',\n",
       "       'Common Applications of CNNs',\n",
       "       'Challenges of Image Classification',\n",
       "       'Convolutional Neural Network Basics',\n",
       "       'Convolutional Filters and Weight-Sharing',\n",
       "       'Cross-correlation vs. Convolution', 'CNNs & Backpropagation',\n",
       "       'CNN Architectures & AlexNet', 'What a CNN Can See',\n",
       "       'LeNet-5 in PyTorch', 'Saving and Loading Models in PyTorch',\n",
       "       'AlexNet in PyTorch',\n",
       "       'Convolutional Neural Networks Architectures',\n",
       "       'Convolutions and Padding', 'Spatial Dropout and BatchNorm',\n",
       "       'Architecture Overview', 'VGG16 Overview', 'VGG16 in PyTorch',\n",
       "       'ResNet Overview', 'ResNet-34 in PyTorch',\n",
       "       'Replacing Max-Pooling with Convolutional Layers',\n",
       "       'All-Convolutional Network in PyTorch',\n",
       "       'Convolutional Instead of Fully Connected Layers',\n",
       "       'Transfer Learning', 'Transfer Learning in PyTorch',\n",
       "       'Introduction to Recurrent Neural Networks',\n",
       "       'Different Methods for Working With Text Data',\n",
       "       'Sequence Modeling with RNNs',\n",
       "       'Different Types of Sequence Modeling Tasks',\n",
       "       'Backpropagation Through Time Overview', 'Long Short-Term Memory',\n",
       "       'RNNs for Classification: A Many-to-One Word RNN',\n",
       "       'An RNN Sentiment Classifier in PyTorch',\n",
       "       'Introduction to Autoencoders', 'Dimensionality Reduction',\n",
       "       'A Fully-Connected Autoencoder',\n",
       "       'Convolutional Autoencoders & Transposed Convolutions',\n",
       "       'A Convolutional Autoencoder in PyTorch',\n",
       "       'Other Types of Autoencoders', 'Intro to Variational Autoencoders',\n",
       "       'Variational Autoencoder Overview',\n",
       "       'Sampling from a Variational Autoencoder', 'The Log-Var Trick',\n",
       "       'Variational Autoencoder Loss Function',\n",
       "       'A Variational Autoencoder for Handwritten Digits in PyTorch',\n",
       "       'A Variational Autoencoder for Face Images in PyTorch ',\n",
       "       'VAE Latent Space Arithmetic in PyTorch',\n",
       "       'Introduction to Generative Adversarial Networks ',\n",
       "       'The Main Idea Behind GANs', 'The GAN Objective',\n",
       "       'Modifying the GAN Loss Function for Practical Use',\n",
       "       'A GAN for Generating Handwritten Digits in PyTorch',\n",
       "       'Tips and Tricks to Make GANs Work',\n",
       "       'A DCGAN for Generating Face Images in PyTorch',\n",
       "       'RNNs & Transformers for Sequence-to-Sequence Modeling',\n",
       "       'Sequence Generation with Word and Character RNNs',\n",
       "       'Implementing a Character RNN in PyTorch',\n",
       "       'RNNs with an Attention Mechanism',\n",
       "       'Using Attention Without the RNN. A Basic Form of Self-Attention',\n",
       "       'Self-Attention and Scaled Dot-Product Attention',\n",
       "       'Multi-Head Attention', 'The Transformer Architecture',\n",
       "       'Some Popular Transformer Models: BERT, GPT, and BART',\n",
       "       'GPT-v1: Generative Pre-Trained Transformer',\n",
       "       'BERT: Bidirectional Encoder Representations from Transformers',\n",
       "       'GPT-v2: Language Models are Unsupervised Multitask Learners',\n",
       "       'GPT-v3: Language Models are Few-Shot Learners',\n",
       "       'BART: Combining Bidirectional and Auto-Regressive Transformers',\n",
       "       'The Recent Growth of Language Transformers',\n",
       "       'DistilBert Movie Review Classifier in PyTorch'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[' Topic'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df.Time[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in df.index:\n",
    "    str_a = df['Time'][idx]\n",
    "    df['Time'][idx] = ''.join(('0:', str_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Time'] = pd.to_timedelta(df.Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('1 days 11:42:47')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Time.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 32-bit",
   "language": "python",
   "name": "python37432bitc856d1d617f0478da8cbf97a005d9730"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
